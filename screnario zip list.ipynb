{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207042d-eefb-42b2-96bf-9a7728cb9ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are given a list of values and a list of column names. Your task is to group the values into subsets, each corresponding to the length of the column names list. Then, you need to create a list of dictionaries where each dictionary maps the column names to the values in each subset. If a subset has fewer elements than the number of columns, the remaining keys should be assigned None. Implement this solution using PySpark DataFrame API without explicitly sorting the values.\n",
    "\n",
    "# For example, given values = [1, 2, 3, 4, 5, 6, 7, 8, 9] and columns = ['a', 'b', 'c'], the expected output is:\n",
    "\n",
    "# python\n",
    "# Copy code\n",
    "# [\n",
    "#     {'a': 1, 'b': 2, 'c': 3}, \n",
    "#     {'a': 4, 'b': 5, 'c': 6}, \n",
    "#     {'a': 7, 'b': 8, 'c': 9}\n",
    "# ]\n",
    "# Demonstrate how you would achieve this using PySpark's DataFrame API, ensuring that the original order of the values is preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995d6e98-4355-41dd-8bd0-b4d06bdc876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ZipLists\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "384ed9b5-cca8-45b2-aeae-a5f9b93f5405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+\n",
      "|value|  unique_id|index|\n",
      "+-----+-----------+-----+\n",
      "|  101|          0|    1|\n",
      "|  402|          1|    2|\n",
      "|  423| 8589934592|    3|\n",
      "|   64| 8589934593|    4|\n",
      "|   52|17179869184|    5|\n",
      "|   66|17179869185|    6|\n",
      "| 1763|25769803776|    7|\n",
      "| 2208|25769803777|    8|\n",
      "| 4199|25769803778|    9|\n",
      "+-----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example data\n",
    "values = [101, 402, 423, 64, 52, 66, 1763, 2208, 4199]\n",
    "col_names = ['a', 'b', 'c']\n",
    "\n",
    "# Convert the list of values into a DataFrame with an index\n",
    "# df = spark.createDataFrame([(v,) for v in values], [\"value\"]).withColumn(\"index\", row_number().over(Window.orderBy(\"value\")))\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "# Convert the list of values into a DataFrame\n",
    "df = spark.createDataFrame([(v,) for v in values], [\"value\"])\n",
    "\n",
    "# Add a unique ID to each row without ordering by value\n",
    "df = df.withColumn(\"unique_id\", monotonically_increasing_id())\n",
    "\n",
    "# Calculate the number of values per group based on the length of col_names\n",
    "subset_size = len(col_names)\n",
    "\n",
    "window_spec = Window.orderBy(\"unique_id\")\n",
    "# Calculate the row number based on the unique_id within each partition (essentially creating an index)\n",
    "df = df.withColumn(\"index\", row_number().over(window_spec))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8595fc98-21c1-420d-8800-f586c58d4551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+-----+\n",
      "|value|  unique_id|index|group|\n",
      "+-----+-----------+-----+-----+\n",
      "|  101|          0|    1|    0|\n",
      "|  402|          1|    2|    0|\n",
      "|  423| 8589934592|    3|    0|\n",
      "|   64| 8589934593|    4|    1|\n",
      "|   52|17179869184|    5|    1|\n",
      "|   66|17179869185|    6|    1|\n",
      "| 1763|25769803776|    7|    2|\n",
      "| 2208|25769803777|    8|    2|\n",
      "| 4199|25769803778|    9|    2|\n",
      "+-----+-----------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the group number based on the length of col_names\n",
    "grouped_df = df.withColumn(\"group\", expr(f\"(index - 1) div {len(col_names)}\"))\n",
    "\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0cc2e97-d376-439f-92e3-4cb9b2fb42e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----+-----+----+----+----+\n",
      "|value|  unique_id|index|group|   a|   b|   c|\n",
      "+-----+-----------+-----+-----+----+----+----+\n",
      "|  101|          0|    1|    0| 101| 402| 423|\n",
      "|  402|          1|    2|    0| 402| 423|NULL|\n",
      "|  423| 8589934592|    3|    0| 423|NULL|NULL|\n",
      "|   64| 8589934593|    4|    1|  64|  52|  66|\n",
      "|   52|17179869184|    5|    1|  52|  66|NULL|\n",
      "|   66|17179869185|    6|    1|  66|NULL|NULL|\n",
      "| 1763|25769803776|    7|    2|1763|2208|4199|\n",
      "| 2208|25769803777|    8|    2|2208|4199|NULL|\n",
      "| 4199|25769803778|    9|    2|4199|NULL|NULL|\n",
      "+-----+-----------+-----+-----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add column names based on their position in the group\n",
    "for i, name in enumerate(col_names):\n",
    "    grouped_df = grouped_df.withColumn(name, expr(f\"lead(value, {i}) over (partition by group order by index)\"))\n",
    "\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ebe6bfe-9261-4f4f-b9de-634ee745c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|   a|   b|   c|\n",
      "+----+----+----+\n",
      "| 101| 402| 423|\n",
      "|  64|  52|  66|\n",
      "|1763|2208|4199|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter only the first row of each group and select the columns\n",
    "result_df = grouped_df.filter(expr(f\"index % {len(col_names)} = 1\")).select(col_names)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "918362ad-44fb-4d36-86f1-ecd06628a6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 101, 'b': 402, 'c': 423}, {'a': 64, 'b': 52, 'c': 66}, {'a': 1763, 'b': 2208, 'c': 4199}]\n"
     ]
    }
   ],
   "source": [
    "# Convert each row to a dictionary\n",
    "result = result_df.rdd.map(lambda row: row.asDict()).collect()\n",
    "\n",
    "# Show the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c5b76-001d-4207-84cd-f1d0f0c9242b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
